{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO+s4z0S2Y970yRgt5xkuQo"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aYKYUJzbP6iZ","executionInfo":{"status":"ok","timestamp":1685598420692,"user_tz":-330,"elapsed":1457,"user":{"displayName":"Khushi Gondane","userId":"11753380767572050077"}},"outputId":"347293b1-5667-4f54-f68c-ddc20b91ec27"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1., grad_fn=<PowBackward0>)\n","tensor(-2.)\n"]}],"source":["# backpropagation\n","import torch\n","\n","x = torch.tensor(1.0)\n","y = torch.tensor(2.0)\n","\n","# This is the parameter we want to optimize -> requires_grad=True\n","w = torch.tensor(1.0, requires_grad=True)\n","# forward pass to compute loss\n","y_predicted = w * x\n","loss = (y_predicted - y)**2\n","print(loss)\n","\n","# backward pass to compute gradient dLoss/dw\n","loss.backward()\n","print(w.grad)\n"]},{"cell_type":"code","source":["\n","# update weights\n","# next forward and backward pass...\n","\n","# continue optimizing:\n","# update weights, this operation should not be part of the computational graph\n","with torch.no_grad():\n","    w -= 0.01 * w.grad\n","# don't forget to zero the gradients\n","w.grad.zero_()\n","\n","# next forward and backward pass...\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vxjnKOsGflo9","executionInfo":{"status":"ok","timestamp":1685598594558,"user_tz":-330,"elapsed":681,"user":{"displayName":"Khushi Gondane","userId":"11753380767572050077"}},"outputId":"1603dedd-94fc-4393-db30-683a037428e5"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0.)"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["import numpy as np \n","\n","# Compute every step manually\n","\n","# Linear regression\n","# f = w * x \n","\n","# here : f = 2 * x\n","X = np.array([1, 2, 3, 4], dtype=np.float32)\n","Y = np.array([2, 4, 6, 8], dtype=np.float32)\n","\n","w = 0.0\n","\n","# model output\n","def forward(x):\n","    return w * x\n","\n","# loss = MSE\n","def loss(y, y_pred):\n","    return ((y_pred - y)**2).mean()\n","\n","# J = MSE = 1/N * (w*x - y)**2\n","# dJ/dw = 1/N * 2x(w*x - y)\n","def gradient(x, y, y_pred):\n","    return np.mean(2*x*(y_pred - y))\n","\n","print(f'Prediction before training: f(5) = {forward(5):.3f}')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vEeSk58Eg4Hz","executionInfo":{"status":"ok","timestamp":1685598745593,"user_tz":-330,"elapsed":561,"user":{"displayName":"Khushi Gondane","userId":"11753380767572050077"}},"outputId":"23415d4c-7f63-49bd-e9b8-2c821dcd116d"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction before training: f(5) = 0.000\n"]}]},{"cell_type":"code","source":["\n","# Training\n","learning_rate = 0.01\n","n_iters = 20\n","\n","for epoch in range(n_iters):\n","    # predict = forward pass\n","    y_pred = forward(X)\n","\n","    # loss\n","    l = loss(Y, y_pred)\n","    \n","    # calculate gradients\n","    dw = gradient(X, Y, y_pred)\n","\n","    # update weights\n","    w -= learning_rate * dw\n","\n","    if epoch % 2 == 0:\n","        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:.8f}')\n","     \n","print(f'Prediction after training: f(5) = {forward(5):.3f}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"la8UDtoWliHE","executionInfo":{"status":"ok","timestamp":1685599961379,"user_tz":-330,"elapsed":579,"user":{"displayName":"Khushi Gondane","userId":"11753380767572050077"}},"outputId":"c6b3e392-3949-40ee-9cc4-a9e6e85356d1"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["epoch 1: w = 0.300, loss = 30.00000000\n","epoch 3: w = 0.772, loss = 15.66018677\n","epoch 5: w = 1.113, loss = 8.17471600\n","epoch 7: w = 1.359, loss = 4.26725292\n","epoch 9: w = 1.537, loss = 2.22753215\n","epoch 11: w = 1.665, loss = 1.16278565\n","epoch 13: w = 1.758, loss = 0.60698175\n","epoch 15: w = 1.825, loss = 0.31684822\n","epoch 17: w = 1.874, loss = 0.16539653\n","epoch 19: w = 1.909, loss = 0.08633806\n","Prediction after training: f(5) = 9.612\n"]}]},{"cell_type":"code","source":["# 1) Design model (input, output, forward pass with different layers)\n","# 2) Construct loss and optimizer\n","# 3) Training loop\n","#       - Forward = compute prediction and loss\n","#       - Backward = compute gradients\n","#       - Update weights\n","\n","import torch\n","import torch.nn as nn\n","\n","# Linear regression\n","# f = w * x \n","\n","# here : f = 2 * x\n","\n","# 0) Training samples\n","X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n","Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n","\n","# 1) Design Model: Weights to optimize and forward function\n","w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n","\n","def forward(x):\n","    return w * x\n","\n","print(f'Prediction before training: f(5) = {forward(5).item():.3f}')\n","\n","# 2) Define loss and optimizer\n","learning_rate = 0.01\n","n_iters = 100\n","\n","# callable function\n","loss = nn.MSELoss()\n","\n","optimizer = torch.optim.SGD([w], lr=learning_rate)\n","\n","# 3) Training loop\n","for epoch in range(n_iters):\n","    # predict = forward pass\n","    y_predicted = forward(X)\n","\n","    # loss\n","    l = loss(Y, y_predicted)\n","\n","    # calculate gradients = backward pass\n","    l.backward()\n","\n","    # update weights\n","    optimizer.step()\n","\n","    # zero the gradients after updating\n","    optimizer.zero_grad()\n","\n","    if epoch % 10 == 0:\n","        print('epoch ', epoch+1, ': w = ', w, ' loss = ', l)\n","\n","print(f'Prediction after training: f(5) = {forward(5).item():.3f}')\n"],"metadata":{"id":"UjojIePEDa9n","executionInfo":{"status":"ok","timestamp":1685607796179,"user_tz":-330,"elapsed":2590,"user":{"displayName":"Khushi Gondane","userId":"11753380767572050077"}},"outputId":"a37764f8-cabb-4b11-faa4-83cafaf2c928","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction before training: f(5) = 0.000\n","epoch  1 : w =  tensor(0.3000, requires_grad=True)  loss =  tensor(30., grad_fn=<MseLossBackward0>)\n","epoch  11 : w =  tensor(1.6653, requires_grad=True)  loss =  tensor(1.1628, grad_fn=<MseLossBackward0>)\n","epoch  21 : w =  tensor(1.9341, requires_grad=True)  loss =  tensor(0.0451, grad_fn=<MseLossBackward0>)\n","epoch  31 : w =  tensor(1.9870, requires_grad=True)  loss =  tensor(0.0017, grad_fn=<MseLossBackward0>)\n","epoch  41 : w =  tensor(1.9974, requires_grad=True)  loss =  tensor(6.7705e-05, grad_fn=<MseLossBackward0>)\n","epoch  51 : w =  tensor(1.9995, requires_grad=True)  loss =  tensor(2.6244e-06, grad_fn=<MseLossBackward0>)\n","epoch  61 : w =  tensor(1.9999, requires_grad=True)  loss =  tensor(1.0176e-07, grad_fn=<MseLossBackward0>)\n","epoch  71 : w =  tensor(2.0000, requires_grad=True)  loss =  tensor(3.9742e-09, grad_fn=<MseLossBackward0>)\n","epoch  81 : w =  tensor(2.0000, requires_grad=True)  loss =  tensor(1.4670e-10, grad_fn=<MseLossBackward0>)\n","epoch  91 : w =  tensor(2.0000, requires_grad=True)  loss =  tensor(5.0768e-12, grad_fn=<MseLossBackward0>)\n","Prediction after training: f(5) = 10.000\n"]}]}]}